Enhancement Scenario: Transaction Report Generator

Context

You are inheriting a small internal command-line tool used by analysts to inspect daily payment transactions. The tool is a single Python script:

    transaction_reporter.py

It reads a CSV file of raw transactions and prints a short plain-text summary to stdout. The script works and is used in production, but it is hard to extend, inefficient on large files, and its output is not easy to reuse from other systems.

You have been asked to enhance this tool while preserving the behavior current users rely on.

Baseline Behavior (Do Not Break)

The existing script exposes two entry points:

- Function:

    def generate_report(csv_path: str) -> dict

- CLI:

    python transaction_reporter.py path/to/transactions.csv

Given a CSV file where each row is a single transaction, the baseline implementation:

- Uses only the Python standard library.
- Reads the entire file into memory.
- Parses a fixed set of columns (id, timestamp, amount_cents, currency, status).
- Computes summary metrics, such as:
  - total number of rows
  - number of completed and failed transactions
  - sum of amount_cents for completed transactions
  - overall average amount_cents
- Returns these aggregates from generate_report as a flat dict of simple scalar values.
- Prints a human-readable summary to stdout when invoked from the command line. The content of this default summary (metrics and numbers) is part of the contract.

Assume the baseline script already exists and behaves correctly for well-formed CSV inputs.

Core Objective

Enhance the transaction reporting tool to make it more efficient, modular, and integration-friendly, without changing observable semantics:

- For the same CSV input, generate_report must return the same aggregate values.
- With default options, the CLI summary must remain semantically equivalent (same metrics and values).
- The tool should be easier to extend with new metrics, output formats, and programmatic consumers.

Baseline Pain Points

- Parsing, aggregation, formatting, and CLI handling live in one tightly coupled script.
- The implementation always loads the full CSV into memory.
- Business logic and presentation are mixed.
- There is no structured output format for dashboards or automation.
- Error handling and diagnostics are minimal.

Enhancement Directions

1) Structure and Modularity

- Split responsibilities into components (names are suggestions, not strict):
  - Reader: stream transactions from a CSV file.
  - Core/aggregator: compute metrics from an iterator of transactions.
  - Formatters: render an in-memory report into specific output formats.
  - CLI layer: argument parsing, configuration, and wiring.
- Keep generate_report(csv_path: str) -> dict as a stable entry point that delegates into the new architecture.

2) Output Flexibility

- Define an internal structured representation of a report (for example, a nested dict).
- Preserve a default text summary mode that matches the baseline behavior when no extra flags are provided.
- Add at least one extra output mode for machine consumption (e.g., JSON), selectable via CLI and programmatic APIs.
- Ensure all formats are rendered from the same report data.

3) Performance and Scalability

- Where reasonable, replace "read everything into a list" with streaming-style processing when reading the CSV.
- Keep memory usage acceptable for large transaction files.
- Design the architecture so that more advanced strategies (such as batching) could be added later without a rewrite.

4) Robustness and Observability

- Improve handling of common failures (missing files, malformed rows, invalid numeric values) while preserving behavior for valid inputs.
- Expose basic observability data (such as counters or simple timings) that tests can assert on.
- Avoid noisy logging by default; extra diagnostics should be opt-in.

Hard Constraints

- Programmatic API:
  - generate_report(csv_path: str) -> dict remains supported and must return aggregates consistent with the baseline.
  - You may add richer APIs that expose the full structured report, but must not remove or weaken the original.
- CLI behavior:
  - Running "python transaction_reporter.py path/to/file.csv" must still print a human-readable summary comparable to the baseline.
  - New CLI options (for example, output format or extra metrics) are allowed only if the default behavior remains unchanged.
- Data handling:
  - Interpretation of existing columns must not change.
  - Default-path aggregates (counts, sums, averages) must remain consistent for well-formed input.

Testing and Validation

- Add automated tests that cover both the programmatic API and CLI behavior.
- Include scenarios for normal inputs, simple error conditions, and new output modes.
- Use tests to guard the semantic guarantees above and to make future refactors safer.

One-Command Test Runner

- Add a single entry script at the repository root:

    run_tests

- The script must:
  - Be executable via: ./run_tests
  - Use only the Python standard library to prepare the environment.
  - Run the full test suite (pytest if available, otherwise unittest or another standard runner).
  - Exit with code 0 only if all tests pass.

Reproducible Environment

- Document the required Python version (for example, Python 3.10+) and any runtime assumptions in README.md.
- Assume the tool runs on a Windows environment (for example, Windows 10 or later).
- Do not rely on external system tools or third-party libraries.
- A clean machine should be able to run the project following only the README instructions.

Documentation

- Create or update README.md to include:
  - A short description of the transaction reporting tool.
  - A summary of the original limitations and how your design addresses them.
  - An overview of the new internal architecture (modules and main responsibilities).
  - Instructions for environment setup and running tests via ./run_tests.
  - At least one before/after example showing the original summary output and one enhanced output mode for the same sample CSV.

Treat this as an enhancement to a live internal tool: preserve the behavior existing users rely on while making the implementation cleaner, more maintainable, and easier to evolve. Let your tests demonstrate that core semantics remain unchanged.